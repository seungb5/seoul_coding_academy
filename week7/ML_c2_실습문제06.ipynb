{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mglearn\n",
    "from sklearn.base import clone\n",
    "from matplotlib import font_manager\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "# font_name = font_manager.FontProperties(fname=\"NanumBarunGothic.ttf\").get_name()\n",
    "plt.rc('font', family=font_name)\n",
    "plt.rcParams[\"font.family\"] = font_name\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameter 조건\n",
    "#### 1. RFC\n",
    "- cancer 데이터를 이용하여:\n",
    "- RandomForestClassifier \n",
    "- n_estimators=100, random_state=0\n",
    "\n",
    "- max_depth=[1, 2, 3, 4, 5]\n",
    "- max_features = .5 'auto'\n",
    "- best 모델과 best 파라미터를 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. \n",
    "- GradientBoostingClassifier\n",
    "- n_estimators=50, random_state=0\n",
    "\n",
    "- learning_rate=[.2, .1, .05], \n",
    "- subsample = [.1, .5], \n",
    "- max_depth=[1, 2, 3, 4, 5], \n",
    "- max_features = [1., .5]\n",
    "- best 모델과 best 파라미터를 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. \n",
    "- best model에 대한 warm_start로 n_estimators=100 학습 score룰 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. \n",
    "- init으로 RandomForestClassifier의 best 파라미터로 n_estimators=100 학습하여 score를 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습 06\n",
    "#### 1. RFC best model, parameter\n",
    "#### 2. GBC best model, parameter\n",
    "#### 3.  best model에 대한 warm_start로 n_estimators = 100 학습\n",
    "#### 4. init으로 RFC의 best 모델로 n_estimators = 100 학습\n",
    "\n",
    "- 위 4개 모델의 score 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_boston\n",
    "# boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=1, max_features=0.5, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.939\n",
      "테스트 세트 정확도: 0.951\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=1, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.930\n",
      "테스트 세트 정확도: 0.944\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features=0.5, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.965\n",
      "테스트 세트 정확도: 0.958\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.960\n",
      "테스트 세트 정확도: 0.958\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features=0.5, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.988\n",
      "테스트 세트 정확도: 0.979\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.981\n",
      "테스트 세트 정확도: 0.958\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features=0.5, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.993\n",
      "테스트 세트 정확도: 0.958\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.993\n",
      "테스트 세트 정확도: 0.965\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features=0.5, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.993\n",
      "테스트 세트 정확도: 0.979\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.993\n",
      "테스트 세트 정확도: 0.972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# 0. 데이터 불러오기\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# 1. 데이터 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "#### 1\n",
    "RFC = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "maxdepth = [1, 2, 3, 4, 5]\n",
    "maxfeatures = [.5, 'auto']\n",
    "\n",
    "for depth in maxdepth:\n",
    "    for feature in maxfeatures:\n",
    "        RFC.set_params(max_features=feature, max_depth=depth)\n",
    "        RFC.fit(X_train, y_train)\n",
    "        print(\"{} :\\n\".format(RFC))\n",
    "        print(\"훈련 세트 정확도: {:.3f}\".format(RFC.score(X_train, y_train)))\n",
    "        print(\"테스트 세트 정확도: {:.3f}\\n\".format(RFC.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features=0.5, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.988\n",
      "테스트 세트 정확도: 0.979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_RFC = RandomForestClassifier(n_estimators=100, random_state=0, max_depth=3, max_features=0.5)\n",
    "best_RFC.fit(X_train, y_train)\n",
    "print(\"{} :\\n\".format(best_RFC))\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(best_RFC.score(X_train, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\\n\".format(best_RFC.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best parameter\n",
    "    - max_depth=3\n",
    "    - max_features=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.2, loss='deviance', max_depth=3,\n",
      "              max_features=1.0, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              n_iter_no_change=None, presort='auto', random_state=0,\n",
      "              subsample=0.5, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 1.000\n",
      "테스트 세트 정확도: 0.986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 2\n",
    "gbrt = GradientBoostingClassifier(n_estimators=50, random_state=0)\n",
    "learning_rate= [.2, .1, .05]\n",
    "subsample = [.1, .5]\n",
    "max_depth= [1, 2, 3, 4, 5]\n",
    "max_features = [1., .5]\n",
    "\n",
    "for mf in max_features:\n",
    "    for lr in learning_rate:\n",
    "        for ss in subsample:\n",
    "            for md in max_depth:\n",
    "                gbrt.set_params(max_features=mf, max_depth=md, subsample=ss, learning_rate=lr)\n",
    "                gbrt.fit(X_train, y_train)\n",
    "                if gbrt.score(X_test, y_test) > .98:\n",
    "                    print(\"{} :\\n\".format(gbrt))\n",
    "                    print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "                    print(\"테스트 세트 정확도: {:.3f}\\n\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.2, loss='deviance', max_depth=3,\n",
      "              max_features=1.0, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              n_iter_no_change=None, presort='auto', random_state=0,\n",
      "              subsample=0.5, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 1.000\n",
      "테스트 세트 정확도: 0.986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_gbrt = GradientBoostingClassifier(n_estimators=50, random_state=0, subsample=0.5, max_features=1.0, max_depth=3, learning_rate=0.2)\n",
    "best_gbrt.fit(X_train, y_train)\n",
    "print(\"{} :\\n\".format(best_gbrt))\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(best_gbrt.score(X_train, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\\n\".format(best_gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best parameter\n",
    "    - learning_rate=0.2\n",
    "    - max_depth=3\n",
    "    - max_features=1.0\n",
    "    - subsample=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=0,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0,\n",
      "              warm_start=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.2, loss='deviance', max_depth=3,\n",
      "              max_features=1.0, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_sampl...      subsample=0.5, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)) :\n",
      "\n",
      "훈련 세트 정확도: 1.000\n",
      "테스트 세트 정확도: 0.958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 3\n",
    "warm_gbrt = GradientBoostingClassifier(n_estimators=100, random_state=0, warm_start=best_gbrt)\n",
    "warm_gbrt.fit(X_train, y_train)\n",
    "print(\"{} :\\n\".format(warm_gbrt))\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(warm_gbrt.score(X_train, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\\n\".format(warm_gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse',\n",
       "              init=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=3, max_features=0.5, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=0,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0,\n",
       "              warm_start=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.2, loss='deviance', max_depth=3,\n",
       "              max_features=1.0, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=0.5, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4\n",
    "init_gbrt = clone(warm_gbrt)\n",
    "init_gbrt.set_params(init = best_RFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- init에 best_RFC가 들어가지만, fitting을 할 때, 생성자 함수가 겹쳐서 제대로 동작하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_gbrt.fit(X_train, y_train)\n",
    "# print(\"{} :\\n\".format(init_gbrt))\n",
    "# print(\"훈련 세트 정확도: {:.3f}\".format(init_gbrt.score(X_train, y_train)))\n",
    "# print(\"테스트 세트 정확도: {:.3f}\\n\".format(init_gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features=0.5, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 0.988\n",
      "테스트 세트 정확도: 0.979\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.2, loss='deviance', max_depth=3,\n",
      "              max_features=1.0, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              n_iter_no_change=None, presort='auto', random_state=0,\n",
      "              subsample=0.5, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False) :\n",
      "\n",
      "훈련 세트 정확도: 1.000\n",
      "테스트 세트 정확도: 0.986\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=0,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0,\n",
      "              warm_start=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.2, loss='deviance', max_depth=3,\n",
      "              max_features=1.0, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_sampl...      subsample=0.5, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)) :\n",
      "\n",
      "훈련 세트 정확도: 1.000\n",
      "테스트 세트 정확도: 0.958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.\n",
    "models = [best_RFC, best_gbrt, warm_gbrt]\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"{} :\\n\".format(model))\n",
    "    print(\"훈련 세트 정확도: {:.3f}\".format(model.score(X_train, y_train)))\n",
    "    print(\"테스트 세트 정확도: {:.3f}\\n\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# read in data\n",
    "train = xgb.DMatrix(X_train)\n",
    "\n",
    "test = xgb.DMatrix('demo/data/agaricus.txt.test')\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(bst.score(X_train, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\\n\".format(bst.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = X1 \n",
    "RF = 100개의 나무\n",
    "GB = X1 + e1\n",
    "GB = X1 + X2 + e2\n",
    "GB = x1 + x2 + x3 + e3\n",
    "GB = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
